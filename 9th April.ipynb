{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09b4fbcb-a235-4ae4-a7a1-d1b337be924d",
   "metadata": {},
   "source": [
    "# 1] What is Bayes' theorem?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b8ff87-580b-47e5-8686-2c9d7c560859",
   "metadata": {},
   "source": [
    "\n",
    "### => Bayes' theorem is a fundamental concept in probability theory and statistics, named after the Reverend Thomas Bayes. It provides a way to update our beliefs or knowledge about an event based on new evidence or information.\n",
    "\n",
    "### Mathematically, Bayes' theorem can be stated as follows:\n",
    "\n",
    "### P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "### Where:\n",
    "\n",
    "### P(A|B) represents the probability of event A occurring given that event B has already occurred.\n",
    "### P(B|A) is the probability of event B occurring given that event A has already occurred.\n",
    "### P(A) is the prior probability of event A occurring, which is our initial belief or knowledge about the probability of A.\n",
    "### P(B) is the prior probability of event B occurring.\n",
    "### => In simpler terms, Bayes' theorem allows us to calculate the probability of event A happening given that we have observed event B, by taking into account the prior probability of A and the likelihood of observing B if A were true.\n",
    "\n",
    "### => Bayes' theorem is widely used in various fields, including statistics, machine learning, artificial intelligence, and data analysis. It enables us to make probabilistic inferences, update our beliefs with new evidence, and perform Bayesian reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ed2c90-08e7-4e2b-bae1-4c68b595c727",
   "metadata": {},
   "source": [
    "# 2] What is the formula for Bayes' theorem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b37473-22c0-4a85-bf60-a2e469f0613d",
   "metadata": {},
   "source": [
    "### P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "### Where:\n",
    "\n",
    "### P(A|B) represents the probability of event A occurring given that event B has already occurred.\n",
    "### P(B|A) is the probability of event B occurring given that event A has already occurred.\n",
    "### P(A) is the prior probability of event A occurring, which is our initial belief or knowledge about the probability of A.\n",
    "### P(B) is the prior probability of event B occurring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96d99c8-296e-4eef-8867-05ea4d0bdcad",
   "metadata": {},
   "source": [
    "# 3] How is Bayes' theorem used in practice?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d770aea9-e871-42a4-9f5c-ae903db09545",
   "metadata": {},
   "source": [
    "## 1) Medical Diagnosis: \n",
    "### => Bayes' theorem is employed in medical diagnosis to assess the probability of a disease given certain symptoms. It combines the prior probability of the disease (based on its prevalence) with the likelihood of observing those symptoms if the disease is present. This helps doctors update their diagnostic conclusions based on new information.\n",
    "\n",
    "## 2) Spam Filtering:\n",
    "### => Bayes' theorem is used in spam filtering algorithms. It calculates the probability that an incoming email is spam based on the occurrence of specific spam-related words or features in the email. By updating the prior probability of an email being spam with the likelihood of observing those features in spam emails, the filter can make more accurate predictions.\n",
    "\n",
    "## 3) Machine Learning: \n",
    "### => Bayes' theorem is a foundational concept in Bayesian machine learning. It enables the estimation of model parameters based on observed data and prior beliefs. Bayesian models can update their beliefs about the parameters as more data is observed, providing a more robust and flexible approach to learning.\n",
    "\n",
    "## 4) Risk Assessment: \n",
    "### => Bayes' theorem is applied in risk assessment and decision-making under uncertainty. By incorporating prior knowledge and new evidence, it helps estimate the probability of a risk occurring and supports decision-making processes by updating beliefs based on available information.\n",
    "\n",
    "## 5) Fault Diagnosis:\n",
    "### => In engineering and maintenance, Bayes' theorem is used for fault diagnosis. It combines prior probabilities of different faults with observed symptoms or sensor readings to infer the most likely cause of a malfunction or failure in a system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b701723d-5854-470e-9771-01ca67c4ae3b",
   "metadata": {},
   "source": [
    "# 4] What is the relationship between Bayes' theorem and conditional probability?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7fc5a5-b5f9-40f9-ac1b-cf48fba86d90",
   "metadata": {},
   "source": [
    "### => Bayes' theorem is closely related to conditional probability. In fact, Bayes' theorem is derived from conditional probability.\n",
    "\n",
    "### => Conditional probability is a measure of the probability of an event A occurring given that another event B has already occurred. It is denoted as P(A|B), read as \"the probability of A given B.\"\n",
    "\n",
    "### => Bayes' theorem provides a way to calculate conditional probabilities by incorporating prior probabilities and the likelihood of observing the evidence. It states that:\n",
    "\n",
    "### => P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "### => Here, P(B|A) represents the probability of observing event B given that event A has already occurred. P(A) is the prior probability of event A, which is our initial belief or knowledge about the probability of A. P(B) is the prior probability of event B.\n",
    "\n",
    "### => Bayes' theorem allows us to update our belief in event A based on the occurrence of event B. It combines the prior probability of A with the likelihood of observing B if A were true, and normalizes it by the prior probability of B.\n",
    "\n",
    "### => In summary, Bayes' theorem connects conditional probability (P(A|B)) with prior probabilities (P(A) and P(B)) and the likelihood of observing evidence (P(B|A))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28395f5a-c65b-4a7e-b0f9-449075cb592e",
   "metadata": {},
   "source": [
    "# 5] How do you choose which type of Naive Bayes classifier to use for any given problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981a4bcb-9f5a-4cf2-a7ce-446da156f6c6",
   "metadata": {},
   "source": [
    "## 1) Nature of the Features: \n",
    "\n",
    "### Gaussian Naive Bayes: \n",
    "### => This variant assumes that the features follow a Gaussian (normal) distribution. It is suitable for continuous or real-valued features.\n",
    "\n",
    "### Multinomial Naive Bayes:\n",
    "### => This variant is designed for discrete features, such as word frequencies in text classification or occurrences of events in a fixed-size set.\n",
    "\n",
    "### Bernoulli Naive Bayes: \n",
    "### => This variant is specifically suited for binary features, where each feature represents the presence or absence of a particular attribute.\n",
    "## 2) Dataset Size: \n",
    "### Gaussian Naive Bayes:\n",
    "### => It performs well with smaller datasets or when the assumptions of a Gaussian distribution hold reasonably well.\n",
    "\n",
    "### Multinomial Naive Bayes:\n",
    "### => It is often used for large-scale text classification problems with sparse feature vectors, such as document categorization or sentiment analysis.\n",
    "\n",
    "### Bernoulli Naive Bayes:\n",
    "### => It is useful for binary feature datasets with a relatively large number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7980cd9d-1b85-492c-9e4e-9c3ed6999f55",
   "metadata": {},
   "source": [
    "# 6] Assignment:\n",
    "## You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of each feature value for each class:\n",
    "    \n",
    " Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2=4\n",
    " \n",
    " A 3 3 4 4 3 3 3\n",
    " \n",
    " B 2 2 1 2 2 2 3\n",
    "## Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance to belong to?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356e4303-c1f6-4a53-b84c-e1b83e3d7538",
   "metadata": {},
   "source": [
    "### => naive bayes would predict the new instances to belong to class B.\n",
    "### \n",
    "### To determine the class that Naive Bayes would predict for the new instance with features X1 = 3 and X2 = 4, we need to calculate the conditional probabilities for each class.\n",
    "\n",
    "### First, let's calculate the prior probabilities for each class, assuming equal prior probabilities:\n",
    "\n",
    "### P(A) = P(B) = 0.5 (equal prior probabilities for both classes)\n",
    "\n",
    "### Next, we calculate the conditional probabilities using the Naive Bayes assumption of feature independence:\n",
    "\n",
    "### P(X1 = 3 | A) = 4/13\n",
    "### P(X1 = 3 | B) = 1/6\n",
    "\n",
    "### P(X2 = 4 | A) = 3/13\n",
    "### P(X2 = 4 | B) = 3/6\n",
    "\n",
    "### Now, we can calculate the posterior probabilities for each class using Bayes' theorem:\n",
    "\n",
    "### P(A | X1 = 3, X2 = 4) = (P(X1 = 3 | A) * P(X2 = 4 | A) * P(A)) / P(X1 = 3, X2 = 4)\n",
    "### P(B | X1 = 3, X2 = 4) = (P(X1 = 3 | B) * P(X2 = 4 | B) * P(B)) / P(X1 = 3, X2 = 4)\n",
    "\n",
    "### To calculate the denominator P(X1 = 3, X2 = 4), we sum the numerator probabilities for both classes:\n",
    "\n",
    "### P(X1 = 3, X2 = 4) = (P(X1 = 3 | A) * P(X2 = 4 | A) * P(A)) + (P(X1 = 3 | B) * P(X2 = 4 | B) * P(B))\n",
    "\n",
    "### Plugging in the values, we get:\n",
    "\n",
    "### P(A | X1 = 3, X2 = 4) = (4/13 * 3/13 * 0.5) / P(X1 = 3, X2 = 4)\n",
    "### P(B | X1 = 3, X2 = 4) = (1/6 * 3/6 * 0.5) / P(X1 = 3, X2 = 4)\n",
    "\n",
    "### To compare the posterior probabilities, we don't need to calculate the denominator P(X1 = 3, X2 = 4) since it's the same for both classes. We can compare the numerators directly:\n",
    "\n",
    "### P(A | X1 = 3, X2 = 4) = (4/13 * 3/13 * 0.5)\n",
    "### P(B | X1 = 3, X2 = 4) = (1/6 * 3/6 * 0.5)\n",
    "\n",
    "### Calculating these values, we find:\n",
    "\n",
    "### P(A | X1 = 3, X2 = 4) ≈ 0.027 => 41%\n",
    "### P(B | X1 = 3, X2 = 4) ≈ 0.042 => 59%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b653a30-add0-4408-ab28-e0cf71b7a24f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
