{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f843922-fe6a-454f-9cd3-a41781744917",
   "metadata": {},
   "source": [
    "# 1] What is Gradient Boosting Regression?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f543542e-dad5-405d-9e6b-ac4e143940ef",
   "metadata": {},
   "source": [
    "### => Gradient Boosting Regression is a popular machine learning technique used for regression tasks. It is an ensemble learning method that combines the predictions of multiple weak learners (typically decision trees) to create a strong predictive model. The term \"gradient\" in \"Gradient Boosting\" refers to the optimization approach used during the model training process.\n",
    "\n",
    "### Here's a high-level overview of how Gradient Boosting Regression works:\n",
    "\n",
    "## 1) Decision Trees as Weak Learners:\n",
    "### => In Gradient Boosting Regression, decision trees are commonly used as weak learners. A decision tree is a simple tree-like structure that makes a series of decisions based on input features and eventually predicts a target value.\n",
    "\n",
    "## 2) Boosting: \n",
    "### => The boosting technique works by training a sequence of weak learners iteratively. Each weak learner is trained to correct the errors made by its predecessors. The model learns from the mistakes of the previous trees and focuses on examples that were mispredicted.\n",
    "\n",
    "## 3) Residuals:\n",
    "### => During each iteration, the algorithm calculates the difference between the predicted target values and the actual target values of the training data. These differences are known as \"residuals\" or \"pseudo-residuals.\" The next weak learner is then trained to predict these residuals rather than the original target values.\n",
    "\n",
    "## 4) Learning Rate\n",
    "### => To control the contribution of each weak learner, a learning rate (or shrinkage) parameter is used. It scales the contribution of each tree in the ensemble. Lower learning rates usually require more iterations but can result in better generalization.\n",
    "\n",
    "## 5) Aggregation:\n",
    "### => ter training multiple weak learners, their predictions (in the form of residuals) are aggregated to make the final prediction. The predicted values from all the trees are summed up (or averaged) to obtain the final prediction of the Gradient Boosting Regression model.\n",
    "\n",
    "## Gradient Boosting Regression has several advantages:\n",
    "\n",
    "### => It can handle a mix of feature types (e.g., numerical and categorical) without requiring extensive data preprocessing.\n",
    "### => It performs well on a wide range of regression problems.\n",
    "### => It can automatically handle feature interactions, making it more expressive than simple linear models.\n",
    "### => It is less prone to overfitting compared to individual decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d77b5f5-e8a4-41fd-913a-6c905a79b9f1",
   "metadata": {},
   "source": [
    "# 2] Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dc917d9-e4c0-4c81-9c48-15baeb5d06ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bfdb2d6-358b-4d3b-bfb0-311d371481f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=sns.load_dataset(\"iris\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31d804fd-773a-4727-a22a-8b096924b005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   sepal_length  150 non-null    float64\n",
      " 1   sepal_width   150 non-null    float64\n",
      " 2   petal_length  150 non-null    float64\n",
      " 3   petal_width   150 non-null    float64\n",
      " 4   species       150 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 6.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f2fdcb8-a2dc-40e2-a319-c69757cca215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04094851-2e9d-4f93-b7f4-e02fbcaa2d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop(columns=[\"species\"])\n",
    "y=df[\"species\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b717e13-5ba0-4146-a737-b5295910b26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c76d49bb-2a5a-4c23-ba30-f8930585a889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28bd61e1-e4b0-44ab-bd28-ebada5d398bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3e1d316-e9ca-43aa-9691-2e22db4f10ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoostingClf:\n",
    "    def __init__(self, num_trees, max_depth):\n",
    "        self.num_trees = num_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.binary_classifiers = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Create a binary classifier for each class using OvR strategy\n",
    "        classes = np.unique(y)\n",
    "        for target_class in classes:\n",
    "            binary_classifier = DecisionTreeClassifier(max_depth=self.max_depth)\n",
    "            binary_y = np.where(y == target_class, 1, 0)\n",
    "            binary_classifier.fit(X, binary_y)\n",
    "            self.binary_classifiers.append(binary_classifier)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predict the output for a single sample using all binary classifiers\n",
    "        predictions = [clf.predict_proba(X)[:, 1] for clf in self.binary_classifiers]\n",
    "        return np.argmax(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "491ee2f1-559b-46d5-aa79-d0c8439d6198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the categorical target variable to numerical labels\n",
    "class_mapping = {species: idx for idx, species in enumerate(df['species'].unique())}\n",
    "y = np.array([class_mapping[label] for label in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ef53ce2-778b-4a53-931d-7ce3be05adeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.175,random_state=129)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a272907-3b95-4c06-b76a-bcf7969a9f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_clf=GradientBoostingClf(num_trees=100,max_depth=5)\n",
    "gb_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60163bfc-7d2f-48dd-a1c4-4e0589141a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=gb_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "528bb11a-d4e9-43be-8cab-463819f0ad90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       0.75      1.00      0.86         9\n",
      "           2       1.00      0.62      0.77         8\n",
      "\n",
      "    accuracy                           0.89        27\n",
      "   macro avg       0.92      0.88      0.88        27\n",
      "weighted avg       0.92      0.89      0.88        27\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24a08102-ebd8-4c64-8a51-96f087d12ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8888888888888888"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3f9c98-1c9b-40e7-a64b-c7c718b10e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7775a408-d71a-404e-8d0f-a40ec23f234e",
   "metadata": {},
   "source": [
    "# 3] Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "169d8f27-9052-43b7-8c90-b7d213732cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5522ade-9a45-4b9e-bc5e-fbc94cec6828",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94fe69b6-28ab-4197-972a-441e57bb0c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter={\n",
    "    \"learning_rate\":[0.001,0.01,0.1,],\n",
    "    \"n_estimators\":[100,150,200],\n",
    "    \"max_depth\":[2,3,4,5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bbe05d3-c31c-425a-9340-5f67cea6a64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "cv=GridSearchCV(estimator=clf,param_grid=parameter,cv=3,verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0531506e-5e19-4751-9433-9de0c0dbcf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "[CV 1/3] END learning_rate=0.001, max_depth=2, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 2/3] END learning_rate=0.001, max_depth=2, n_estimators=100;, score=0.927 total time=   0.3s\n",
      "[CV 3/3] END learning_rate=0.001, max_depth=2, n_estimators=100;, score=0.976 total time=   0.3s\n",
      "[CV 1/3] END learning_rate=0.001, max_depth=2, n_estimators=150;, score=1.000 total time=   0.4s\n",
      "[CV 2/3] END learning_rate=0.001, max_depth=2, n_estimators=150;, score=0.927 total time=   0.5s\n",
      "[CV 3/3] END learning_rate=0.001, max_depth=2, n_estimators=150;, score=0.976 total time=   0.4s\n",
      "[CV 1/3] END learning_rate=0.001, max_depth=2, n_estimators=200;, score=1.000 total time=   0.5s\n",
      "[CV 2/3] END learning_rate=0.001, max_depth=2, n_estimators=200;, score=0.927 total time=   0.5s\n",
      "[CV 3/3] END learning_rate=0.001, max_depth=2, n_estimators=200;, score=0.976 total time=   0.5s\n",
      "[CV 1/3] END learning_rate=0.001, max_depth=3, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 2/3] END learning_rate=0.001, max_depth=3, n_estimators=100;, score=0.927 total time=   0.3s\n",
      "[CV 3/3] END learning_rate=0.001, max_depth=3, n_estimators=100;, score=0.976 total time=   0.3s\n",
      "[CV 1/3] END learning_rate=0.001, max_depth=3, n_estimators=150;, score=1.000 total time=   0.5s\n",
      "[CV 2/3] END learning_rate=0.001, max_depth=3, n_estimators=150;, score=0.927 total time=   0.4s\n",
      "[CV 3/3] END learning_rate=0.001, max_depth=3, n_estimators=150;, score=0.976 total time=   0.4s\n",
      "[CV 1/3] END learning_rate=0.001, max_depth=3, n_estimators=200;, score=1.000 total time=   0.6s\n",
      "[CV 2/3] END learning_rate=0.001, max_depth=3, n_estimators=200;, score=0.927 total time=   0.5s\n",
      "[CV 3/3] END learning_rate=0.001, max_depth=3, n_estimators=200;, score=0.976 total time=   0.6s\n",
      "[CV 1/3] END learning_rate=0.001, max_depth=4, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 2/3] END learning_rate=0.001, max_depth=4, n_estimators=100;, score=0.927 total time=   0.3s\n",
      "[CV 3/3] END learning_rate=0.001, max_depth=4, n_estimators=100;, score=0.976 total time=   0.3s\n",
      "[CV 1/3] END learning_rate=0.001, max_depth=4, n_estimators=150;, score=1.000 total time=   0.4s\n",
      "[CV 2/3] END learning_rate=0.001, max_depth=4, n_estimators=150;, score=0.927 total time=   0.4s\n",
      "[CV 3/3] END learning_rate=0.001, max_depth=4, n_estimators=150;, score=0.976 total time=   0.5s\n",
      "[CV 1/3] END learning_rate=0.001, max_depth=4, n_estimators=200;, score=1.000 total time=   0.6s\n",
      "[CV 2/3] END learning_rate=0.001, max_depth=4, n_estimators=200;, score=0.927 total time=   0.5s\n",
      "[CV 3/3] END learning_rate=0.001, max_depth=4, n_estimators=200;, score=0.976 total time=   0.6s\n",
      "[CV 1/3] END learning_rate=0.001, max_depth=5, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 2/3] END learning_rate=0.001, max_depth=5, n_estimators=100;, score=0.927 total time=   0.3s\n",
      "[CV 3/3] END learning_rate=0.001, max_depth=5, n_estimators=100;, score=0.976 total time=   0.3s\n",
      "[CV 1/3] END learning_rate=0.001, max_depth=5, n_estimators=150;, score=1.000 total time=   0.4s\n",
      "[CV 2/3] END learning_rate=0.001, max_depth=5, n_estimators=150;, score=0.927 total time=   0.4s\n",
      "[CV 3/3] END learning_rate=0.001, max_depth=5, n_estimators=150;, score=0.976 total time=   0.5s\n",
      "[CV 1/3] END learning_rate=0.001, max_depth=5, n_estimators=200;, score=1.000 total time=   0.6s\n",
      "[CV 2/3] END learning_rate=0.001, max_depth=5, n_estimators=200;, score=0.927 total time=   0.5s\n",
      "[CV 3/3] END learning_rate=0.001, max_depth=5, n_estimators=200;, score=0.976 total time=   0.6s\n",
      "[CV 1/3] END learning_rate=0.01, max_depth=2, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 2/3] END learning_rate=0.01, max_depth=2, n_estimators=100;, score=0.927 total time=   0.3s\n",
      "[CV 3/3] END learning_rate=0.01, max_depth=2, n_estimators=100;, score=0.976 total time=   0.3s\n",
      "[CV 1/3] END learning_rate=0.01, max_depth=2, n_estimators=150;, score=1.000 total time=   0.4s\n",
      "[CV 2/3] END learning_rate=0.01, max_depth=2, n_estimators=150;, score=0.927 total time=   0.4s\n",
      "[CV 3/3] END learning_rate=0.01, max_depth=2, n_estimators=150;, score=0.976 total time=   0.4s\n",
      "[CV 1/3] END learning_rate=0.01, max_depth=2, n_estimators=200;, score=1.000 total time=   0.5s\n",
      "[CV 2/3] END learning_rate=0.01, max_depth=2, n_estimators=200;, score=0.927 total time=   0.5s\n",
      "[CV 3/3] END learning_rate=0.01, max_depth=2, n_estimators=200;, score=0.976 total time=   0.5s\n",
      "[CV 1/3] END learning_rate=0.01, max_depth=3, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 2/3] END learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.927 total time=   0.3s\n",
      "[CV 3/3] END learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.976 total time=   0.3s\n",
      "[CV 1/3] END learning_rate=0.01, max_depth=3, n_estimators=150;, score=1.000 total time=   0.5s\n",
      "[CV 2/3] END learning_rate=0.01, max_depth=3, n_estimators=150;, score=0.927 total time=   0.4s\n",
      "[CV 3/3] END learning_rate=0.01, max_depth=3, n_estimators=150;, score=0.976 total time=   0.4s\n",
      "[CV 1/3] END learning_rate=0.01, max_depth=3, n_estimators=200;, score=1.000 total time=   0.6s\n",
      "[CV 2/3] END learning_rate=0.01, max_depth=3, n_estimators=200;, score=0.927 total time=   0.5s\n",
      "[CV 3/3] END learning_rate=0.01, max_depth=3, n_estimators=200;, score=0.976 total time=   0.6s\n",
      "[CV 1/3] END learning_rate=0.01, max_depth=4, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 2/3] END learning_rate=0.01, max_depth=4, n_estimators=100;, score=0.927 total time=   0.3s\n",
      "[CV 3/3] END learning_rate=0.01, max_depth=4, n_estimators=100;, score=0.976 total time=   0.3s\n",
      "[CV 1/3] END learning_rate=0.01, max_depth=4, n_estimators=150;, score=1.000 total time=   0.4s\n",
      "[CV 2/3] END learning_rate=0.01, max_depth=4, n_estimators=150;, score=0.927 total time=   0.4s\n",
      "[CV 3/3] END learning_rate=0.01, max_depth=4, n_estimators=150;, score=0.976 total time=   0.5s\n",
      "[CV 1/3] END learning_rate=0.01, max_depth=4, n_estimators=200;, score=1.000 total time=   0.6s\n",
      "[CV 2/3] END learning_rate=0.01, max_depth=4, n_estimators=200;, score=0.927 total time=   0.5s\n",
      "[CV 3/3] END learning_rate=0.01, max_depth=4, n_estimators=200;, score=0.976 total time=   0.6s\n",
      "[CV 1/3] END learning_rate=0.01, max_depth=5, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 2/3] END learning_rate=0.01, max_depth=5, n_estimators=100;, score=0.927 total time=   0.3s\n",
      "[CV 3/3] END learning_rate=0.01, max_depth=5, n_estimators=100;, score=0.976 total time=   0.3s\n",
      "[CV 1/3] END learning_rate=0.01, max_depth=5, n_estimators=150;, score=1.000 total time=   0.4s\n",
      "[CV 2/3] END learning_rate=0.01, max_depth=5, n_estimators=150;, score=0.927 total time=   0.4s\n",
      "[CV 3/3] END learning_rate=0.01, max_depth=5, n_estimators=150;, score=0.976 total time=   0.5s\n",
      "[CV 1/3] END learning_rate=0.01, max_depth=5, n_estimators=200;, score=1.000 total time=   0.6s\n",
      "[CV 2/3] END learning_rate=0.01, max_depth=5, n_estimators=200;, score=0.927 total time=   0.5s\n",
      "[CV 3/3] END learning_rate=0.01, max_depth=5, n_estimators=200;, score=0.976 total time=   0.6s\n",
      "[CV 1/3] END learning_rate=0.1, max_depth=2, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 2/3] END learning_rate=0.1, max_depth=2, n_estimators=100;, score=0.927 total time=   0.2s\n",
      "[CV 3/3] END learning_rate=0.1, max_depth=2, n_estimators=100;, score=0.976 total time=   0.3s\n",
      "[CV 1/3] END learning_rate=0.1, max_depth=2, n_estimators=150;, score=1.000 total time=   0.4s\n",
      "[CV 2/3] END learning_rate=0.1, max_depth=2, n_estimators=150;, score=0.927 total time=   0.4s\n",
      "[CV 3/3] END learning_rate=0.1, max_depth=2, n_estimators=150;, score=0.976 total time=   0.4s\n",
      "[CV 1/3] END learning_rate=0.1, max_depth=2, n_estimators=200;, score=1.000 total time=   0.5s\n",
      "[CV 2/3] END learning_rate=0.1, max_depth=2, n_estimators=200;, score=0.927 total time=   0.5s\n",
      "[CV 3/3] END learning_rate=0.1, max_depth=2, n_estimators=200;, score=0.976 total time=   0.6s\n",
      "[CV 1/3] END learning_rate=0.1, max_depth=3, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 2/3] END learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.927 total time=   0.2s\n",
      "[CV 3/3] END learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.976 total time=   0.3s\n",
      "[CV 1/3] END learning_rate=0.1, max_depth=3, n_estimators=150;, score=1.000 total time=   0.4s\n",
      "[CV 2/3] END learning_rate=0.1, max_depth=3, n_estimators=150;, score=0.927 total time=   0.4s\n",
      "[CV 3/3] END learning_rate=0.1, max_depth=3, n_estimators=150;, score=0.976 total time=   0.5s\n",
      "[CV 1/3] END learning_rate=0.1, max_depth=3, n_estimators=200;, score=1.000 total time=   0.6s\n",
      "[CV 2/3] END learning_rate=0.1, max_depth=3, n_estimators=200;, score=0.927 total time=   0.5s\n",
      "[CV 3/3] END learning_rate=0.1, max_depth=3, n_estimators=200;, score=0.976 total time=   0.6s\n",
      "[CV 1/3] END learning_rate=0.1, max_depth=4, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 2/3] END learning_rate=0.1, max_depth=4, n_estimators=100;, score=0.927 total time=   0.2s\n",
      "[CV 3/3] END learning_rate=0.1, max_depth=4, n_estimators=100;, score=0.976 total time=   0.3s\n",
      "[CV 1/3] END learning_rate=0.1, max_depth=4, n_estimators=150;, score=1.000 total time=   0.4s\n",
      "[CV 2/3] END learning_rate=0.1, max_depth=4, n_estimators=150;, score=0.927 total time=   0.4s\n",
      "[CV 3/3] END learning_rate=0.1, max_depth=4, n_estimators=150;, score=0.976 total time=   0.4s\n",
      "[CV 1/3] END learning_rate=0.1, max_depth=4, n_estimators=200;, score=1.000 total time=   0.5s\n",
      "[CV 2/3] END learning_rate=0.1, max_depth=4, n_estimators=200;, score=0.927 total time=   0.5s\n",
      "[CV 3/3] END learning_rate=0.1, max_depth=4, n_estimators=200;, score=0.976 total time=   0.5s\n",
      "[CV 1/3] END learning_rate=0.1, max_depth=5, n_estimators=100;, score=1.000 total time=   0.3s\n",
      "[CV 2/3] END learning_rate=0.1, max_depth=5, n_estimators=100;, score=0.927 total time=   0.2s\n",
      "[CV 3/3] END learning_rate=0.1, max_depth=5, n_estimators=100;, score=0.976 total time=   0.3s\n",
      "[CV 1/3] END learning_rate=0.1, max_depth=5, n_estimators=150;, score=1.000 total time=   0.4s\n",
      "[CV 2/3] END learning_rate=0.1, max_depth=5, n_estimators=150;, score=0.927 total time=   0.4s\n",
      "[CV 3/3] END learning_rate=0.1, max_depth=5, n_estimators=150;, score=0.976 total time=   0.4s\n",
      "[CV 1/3] END learning_rate=0.1, max_depth=5, n_estimators=200;, score=1.000 total time=   0.5s\n",
      "[CV 2/3] END learning_rate=0.1, max_depth=5, n_estimators=200;, score=0.927 total time=   0.5s\n",
      "[CV 3/3] END learning_rate=0.1, max_depth=5, n_estimators=200;, score=0.976 total time=   0.5s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3, estimator=GradientBoostingClassifier(),\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.001, 0.01, 0.1],\n",
       "                         &#x27;max_depth&#x27;: [2, 3, 4, 5],\n",
       "                         &#x27;n_estimators&#x27;: [100, 150, 200]},\n",
       "             verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3, estimator=GradientBoostingClassifier(),\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.001, 0.01, 0.1],\n",
       "                         &#x27;max_depth&#x27;: [2, 3, 4, 5],\n",
       "                         &#x27;n_estimators&#x27;: [100, 150, 200]},\n",
       "             verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3, estimator=GradientBoostingClassifier(),\n",
       "             param_grid={'learning_rate': [0.001, 0.01, 0.1],\n",
       "                         'max_depth': [2, 3, 4, 5],\n",
       "                         'n_estimators': [100, 150, 200]},\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbbd9c46-9273-47f3-965c-fbc5e5013497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.001, 'max_depth': 2, 'n_estimators': 100}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e512cadd-03b6-48b0-9caa-8fa94bb5c624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58877563-4935-4453-9038-c4f20f7b3fa5",
   "metadata": {},
   "source": [
    "# 4] What is a weak learner in Gradient Boosting?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdf64b5-cc93-49ce-a694-c6b780434f7f",
   "metadata": {},
   "source": [
    "### => In the context of Gradient Boosting, a weak learner refers to a simple, relatively low-complexity model that performs only slightly better than random guessing on a given learning task. Weak learners are also often called \"base learners\" or \"base models.\"\n",
    "\n",
    "### => In Gradient Boosting, the weak learners are typically decision trees with limited depth (also known as \"shallow trees\") or decision stumps (trees with a single split). These weak learners are called \"weak\" because their individual predictive power is modest, and they are prone to making errors on the training data.\n",
    "\n",
    "### => The idea behind using weak learners in Gradient Boosting is to combine their predictions in a sequential manner, such that each new learner corrects the errors made by the previous ones. By iteratively adding weak learners to the ensemble, the overall model can become a strong learner with excellent predictive capabilities.\n",
    " \n",
    "### => The key concept of Gradient Boosting is to fit each weak learner to the \"residuals\" of the previous ensemble. Residuals are the differences between the true target values and the predictions made by the current ensemble. By focusing on these residuals, the weak learners can concentrate on the patterns that the previous learners could not capture effectively. This process is repeated for several iterations until the model converges or a predefined number of weak learners are reached.\n",
    "\n",
    "### => The combination of multiple weak learners through boosting allows Gradient Boosting to build powerful and robust predictive models, often outperforming individual strong models or deep decision trees. The success of Gradient Boosting lies in its ability to learn complex patterns and feature interactions by effectively aggregating the knowledge of the weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf926062-de6b-4dab-b41f-9d21008c6a10",
   "metadata": {},
   "source": [
    "# 5] What is the intuition behind the Gradient Boosting algorithm?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759f7d15-ff22-4078-ab1f-34d9a80589ad",
   "metadata": {},
   "source": [
    "\n",
    "### => The intuition behind the Gradient Boosting algorithm can be understood through the analogy of a team of \"experts\" collaborating to solve a problem. Each expert (weak learner) specializes in a specific aspect of the problem but may not be very accurate individually. However, when they work together in a coordinated manner, their collective knowledge leads to a highly accurate and robust solution.\n",
    "\n",
    "### => Here's a step-by-step intuition of how the Gradient Boosting algorithm works:\n",
    "\n",
    "## 1) Initialization:\n",
    "### => Initially, the model starts with an \"ensemble\" that contains just one weak learner, which is often a decision tree with limited depth. This decision tree makes predictions, but it is likely to have significant errors.\n",
    "\n",
    "## 2) Residuals and Learning from Mistakes: \n",
    "### => The algorithm calculates the difference between the true target values and the predictions made by the current ensemble. These differences are the \"residuals\" or \"pseudo-residuals.\" The next weak learner is then trained to focus on and predict these residuals rather than the original target values. The new weak learner is designed to correct the mistakes made by the previous one.\n",
    "\n",
    "## 3) Iterative Learning:\n",
    "### => The algorithm iteratively adds new weak learners to the ensemble, and each new learner continues the process of learning from the residuals of the previous ensemble. At each iteration, the model pays more attention to the examples that were mispredicted in the previous rounds, effectively emphasizing the hard-to-learn patterns.\n",
    "\n",
    "## 4) Weighted Voting:\n",
    "### => The predictions of all weak learners are combined through a weighted voting scheme. The weight of each weak learner depends on its performance and contribution to reducing the overall error of the model. Weak learners that make fewer errors are given more weight in the final prediction.\n",
    "\n",
    "## 5) Gradient Descent Optimization:\n",
    "### => The term \"gradient\" in Gradient Boosting comes from the optimization technique used to minimize the error of the model. The algorithm uses gradient descent (a type of optimization algorithm) to find the optimal direction and magnitude of updates for the ensemble. It moves in the direction of steepest descent in the error landscape, reducing the error at each step.\n",
    "\n",
    "### => By repeating this process for multiple iterations, the ensemble of weak learners gradually improves, and the model converges to a powerful predictive model that captures complex patterns and interactions in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3e5a34-4b26-418f-bf17-3b708780f1a6",
   "metadata": {},
   "source": [
    "# 6] How does Gradient Boosting algorithm build an ensemble of weak learners?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2769d2c-cdc7-4531-b8c9-e07336c820c8",
   "metadata": {},
   "source": [
    "### => The Gradient Boosting algorithm builds an ensemble of weak learners in a sequential manner. It follows a step-by-step process to iteratively improve the ensemble's predictions. Here's a high-level overview of how the ensemble is built:\n",
    "\n",
    "## 1) Initialization: \n",
    "### => The ensemble starts with just one weak learner, typically a decision tree with limited depth. This is the starting point, and its initial predictions are likely to have significant errors.\n",
    "\n",
    "## 2) Compute Residuals:\n",
    "### => The algorithm calculates the difference between the true target values and the predictions made by the current ensemble (or the previous weak learner). These differences are known as \"residuals\" or \"pseudo-residuals.\"\n",
    " \n",
    "## 3) Train a Weak Learner on Residuals:\n",
    "### => The next weak learner is trained to predict the residuals rather than the original target values. In other words, it focuses on learning the patterns in the data that were not captured well by the previous weak learner. The new weak learner is chosen based on the optimization of a loss function (typically the mean squared error for regression tasks), which measures how well it can fit the residuals.\n",
    "\n",
    "## 4) Add Weak Learner to Ensemble:\n",
    "### => Once the new weak learner is trained, it is added to the ensemble. At this point, the ensemble consists of the previously trained weak learners plus the newly added one.\n",
    "\n",
    "## 5) Update Ensemble Predictions:\n",
    "### => The predictions of the ensemble are updated by adding the prediction of the newly added weak learner, multiplied by a \"learning rate\" (or shrinkage) to control the contribution of each weak learner. The learning rate scales the impact of each weak learner, and lower values often lead to better generalization.\n",
    "\n",
    "## 6) Repeat:\n",
    "### => Steps 2 to 5 are repeated for a predefined number of iterations or until a stopping criterion is met (e.g., the model reaches a satisfactory level of performance).\n",
    " \n",
    "## 7) Final Prediction:\n",
    "### => After the last iteration, the final prediction of the Gradient Boosting model is the sum (or average, depending on the algorithm) of the predictions of all weak learners in the ensemble.\n",
    "\n",
    "### => The key idea that drives the success of Gradient Boosting is the sequential training of weak learners to correct the mistakes of the previous ones. Each new weak learner focuses on learning the residuals of the current ensemble, which helps the model gradually improve its predictions. The final ensemble is a combination of multiple weak learners that, together, create a strong predictive model capable of capturing complex patterns and interactions in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce9db19-fc84-46d3-a0a7-44035c52ef16",
   "metadata": {},
   "source": [
    "# 7] What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c385f7-55aa-419a-824f-49c088022c49",
   "metadata": {},
   "source": [
    "### => Constructing the mathematical intuition of the Gradient Boosting algorithm involves understanding the underlying principles and mathematical concepts driving its working. Below are the key steps involved in building the mathematical intuition of Gradient Boosting:\n",
    "\n",
    "## 1)Loss Function: \n",
    "### => The process starts with defining a loss function, which quantifies the error between the model's predictions and the actual target values. For regression tasks, the mean squared error (MSE) is a common choice for the loss function. For classification tasks, functions like log-loss (binary cross-entropy) or softmax cross-entropy are used.\n",
    "\n",
    "## 2)Initial Model: \n",
    "### => The algorithm begins with an initial model, usually a weak learner like a decision tree with limited depth. This model makes predictions, but its accuracy is limited, and it likely has significant errors.\n",
    "\n",
    "## 3)Residuals:\n",
    "### => The algorithm calculates the difference between the true target values and the predictions made by the current ensemble (or the previous weak learner). These differences are the \"residuals\" or \"pseudo-residuals.\"\n",
    "\n",
    "## 4)Training Weak Learners:\n",
    "### => The next step is to train a new weak learner (e.g., another decision tree) on the residuals. This new learner focuses on learning the patterns in the data that were not captured well by the previous weak learner.\n",
    "\n",
    "## 5)Weighted Update:\n",
    "### => The predictions of the new weak learner are multiplied by a \"learning rate\" (often denoted by the symbol eta) before being added to the ensemble's predictions. The learning rate controls the contribution of each weak learner and prevents the model from overfitting. Smaller learning rates generally lead to better generalization.\n",
    "\n",
    "## 6)Update Ensemble Predictions: \n",
    "### => The ensemble's predictions are updated by adding the prediction of the new weak learner (weighted by the learning rate) to the predictions of the previous ensemble. The updated predictions become the new predictions of the ensemble.\n",
    "\n",
    "## 7)Repeat:\n",
    "### => Steps 3 to 6 are repeated for a predefined number of iterations, with each new weak learner focusing on learning the residuals of the current ensemble. The algorithm continues to minimize the loss function and improve the model's predictive performance.\n",
    "\n",
    "## 8)Final Prediction:\n",
    "### => After completing all iterations, the final prediction of the Gradient Boosting model is the sum (or average, depending on the algorithm) of the predictions of all weak learners in the ensemble.\n",
    "\n",
    "## 9)Regularization:\n",
    "### => To prevent overfitting and improve generalization, regularization techniques may be used. Common regularization methods include limiting the depth of individual trees, using a maximum number of trees (n_estimators), and employing subsampling (stochastic gradient boosting).\n",
    "\n",
    "## 10)Prediction for New Data:\n",
    "### => Once the model is trained, it can make predictions on new data by passing the input features through the ensemble of weak learners, aggregating their predictions, and applying the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8004a40b-ea28-4b90-800e-380efc40d5ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
