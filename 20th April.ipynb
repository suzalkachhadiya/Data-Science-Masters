{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b82453f4-1b84-4990-a704-3528ce9beb29",
   "metadata": {},
   "source": [
    "## 1) What is the KNN algorithm?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714a324b-1e8c-4c51-8b0b-eb78ef4940eb",
   "metadata": {},
   "source": [
    "### => The K-Nearest Neighbors (KNN) algorithm is a simple and popular machine learning algorithm used for both classification and regression tasks. It is a non-parametric and instance-based learning algorithm, meaning it doesn't make any assumptions about the underlying data distribution and instead makes predictions based on the similarity of input data points.\n",
    "\n",
    "## 1) Data Preparation: \n",
    "### => First, you need a labeled dataset with features (input data) and corresponding labels (output data). Each data point in the dataset should have a label indicating its class (for classification) or a numerical value (for regression).\n",
    "\n",
    "## 2) Choosing K:\n",
    "### => The \"K\" in KNN represents the number of nearest neighbors to consider when making predictions. It is a hyperparameter that you must choose before applying the algorithm. A common approach is to try different values of K and select the one that gives the best performance on a validation set.\n",
    "\n",
    "## 3) Distance Metric:\n",
    "### => KNN uses a distance metric (e.g., Euclidean distance, Manhattan distance) to measure the similarity between data points. Euclidean distance is often used for continuous features, while other distance metrics may be more appropriate for categorical data.\n",
    "\n",
    "## 4) Prediction (Classification):\n",
    "### => To classify a new data point, the KNN algorithm finds the K nearest neighbors in the training dataset based on the chosen distance metric. It then looks at the class labels of those K neighbors and assigns the class label that appears most frequently among them as the prediction for the new data point.\n",
    "\n",
    "## 5) Prediction (Regression):\n",
    "### => For regression tasks, instead of taking the majority vote of the K nearest neighbors, KNN takes the average (or weighted average) of their target values and assigns it as the prediction for the new data point.\n",
    "\n",
    "## 6) Choosing the Output:\n",
    "### => In some cases, it's essential to handle ties (when two or more classes have the same number of occurrences among the K neighbors) and outliers. These decisions depend on the specific implementation and requirements of the task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c34994-5775-4ad5-b839-1272180785e2",
   "metadata": {},
   "source": [
    "## 2) How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efafac2-1b72-49fb-b6d9-d98230461e58",
   "metadata": {},
   "source": [
    "### => Choosing the right value of K in the K-Nearest Neighbors (KNN) algorithm is a critical aspect of its performance. The value of K directly influences the decision boundary and the accuracy of the model. Selecting an appropriate value for K involves a trade-off between overfitting and underfitting. Here are some common approaches to choose the value of K:\n",
    "\n",
    "## 1) Domain Knowledge:\n",
    "### => If you have domain knowledge about the problem you are trying to solve, it can give you insights into the appropriate value of K. For example, if you know that the decision boundary is likely to be smooth, a larger K might be suitable. On the other hand, if you expect the decision boundary to be complex, a smaller K might be better.\n",
    "\n",
    "## 2) Cross-Validation:\n",
    "### => Cross-validation is a widely used technique to estimate the performance of a model on unseen data. You can use k-fold cross-validation to evaluate the KNN algorithm with different values of K. By splitting your training data into k subsets (folds), training the model on k-1 folds, and validating it on the remaining fold, you can assess the model's performance across various K values. This process can help you identify the value of K that results in the best generalization performance.\n",
    "\n",
    "## 3) Grid Search:\n",
    "### => Grid search is a systematic approach to hyperparameter tuning. In the context of KNN, you can define a range of values for K (e.g., K = 1 to 20), and the algorithm will evaluate the model's performance using each K value. The optimal K will be the one that produces the best performance metric (e.g., accuracy, F1-score) on a validation set.\n",
    "\n",
    "## 4) Elbow Method:\n",
    "### => For classification tasks, you can use the elbow method, which involves plotting the performance metric (e.g., accuracy) against different K values. Typically, the plot will show a decreasing trend in performance as K increases. The \"elbow point\" is the K value where the performance improvement starts to level off. This point is considered a good choice for K, as it balances bias and variance.\n",
    "\n",
    "## 5) Distance Plot:\n",
    "### => Another approach is to plot the distance to the K nearest neighbors against different K values. This can help you understand the data's underlying structure and identify a suitable value for K based on the point where the distances stabilize.\n",
    "\n",
    "## 6) Use Odd K Values:\n",
    "### => When working with binary classification problems, it's often recommended to use odd values of K to avoid ties when taking majority votes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db59506-1681-47f9-93ef-d4a5fd0feb58",
   "metadata": {},
   "source": [
    "## 3) What is the difference between KNN classifier and KNN regressor?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5ba70f-0530-48b1-8d73-8d33ae6e22d8",
   "metadata": {},
   "source": [
    "\n",
    "## **1) KNN Classifier:**\n",
    "\n",
    "## Task: \n",
    "### =>The KNN classifier is used for classification tasks. It is employed when the target variable is categorical, and the goal is to assign a class label to a new input data point based on its similarity to the labeled data points in the training dataset.\n",
    "## Output:\n",
    "### =>The output of the KNN classifier is a class label from the predefined set of classes. The class assigned to the new data point is determined by majority vote among its K nearest neighbors.\n",
    "## Example:\n",
    "### =>For instance, a KNN classifier can be used to classify emails as either \"spam\" or \"not spam\" based on features extracted from the email content.\n",
    "## **2) KNN Regressor:**\n",
    "\n",
    "## Task:\n",
    "### =>The KNN regressor is used for regression tasks. It is employed when the target variable is continuous, and the goal is to predict a numerical value for a new input data point based on the values of its K nearest neighbors in the training dataset.\n",
    "## Output: \n",
    "### =>The output of the KNN regressor is a numerical value that represents the prediction for the new data point. This value is typically computed as the average (or weighted average) of the target values of its K nearest neighbors.\n",
    "## Example:\n",
    "### =>For instance, a KNN regressor can be used to predict the price of a house based on the features of similar houses in the neighborhood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e534a9-6060-4954-848d-d86da2aa017a",
   "metadata": {},
   "source": [
    "## 4) How do you measure the performance of KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98001b8-569b-41a3-b39f-5f2af61e27e6",
   "metadata": {},
   "source": [
    "## **Classification Metrics:**\n",
    "\n",
    "## 1) Accuracy:\n",
    "### => The most straightforward metric, it measures the proportion of correctly classified data points over the total number of data points. It's suitable when the class distribution is balanced.\n",
    "\n",
    "## 2) Precision: \n",
    "### => Precision measures the proportion of true positive predictions (correctly predicted positive class) over the total predicted positive instances (both true positives and false positives). It is useful when the cost of false positives is high.\n",
    "\n",
    "## 3) Recall (Sensitivity or True Positive Rate):\n",
    "### => Recall calculates the proportion of true positive predictions over the total actual positive instances (both true positives and false negatives). It is valuable when the cost of false negatives is high.\n",
    "\n",
    "## 4) F1-Score: \n",
    "### => The F1-score is the harmonic mean of precision and recall. It provides a single value that balances both metrics and is useful when there is an imbalance between classes.\n",
    "\n",
    "## 5) ROC Curve and AUC:\n",
    "### => ROC (Receiver Operating Characteristic) curve visualizes the trade-off between true positive rate (sensitivity) and false positive rate. AUC (Area Under the Curve) summarizes the overall performance of the classifier across different classification thresholds.\n",
    "\n",
    "## **Regression Metrics:**\n",
    "\n",
    "## 1) Mean Squared Error (MSE):\n",
    "### => It calculates the average of the squared differences between predicted and actual target values. Lower MSE indicates better performance.\n",
    "\n",
    "## 2) Root Mean Squared Error (RMSE):\n",
    "### => RMSE is the square root of the MSE and provides an interpretable metric in the original target value units.\n",
    "\n",
    "## 3) Mean Absolute Error (MAE):\n",
    "### => MAE computes the average of the absolute differences between predicted and actual target values. It is less sensitive to outliers than MSE.\n",
    "\n",
    "## 4) R-squared (R2):\n",
    "### => R-squared measures the proportion of the variance in the target variable that is predictable from the input features. It ranges from 0 to 1, with higher values indicating better performance.\n",
    "\n",
    "## **Choosing the Right Metric:**\n",
    "### => The choice of the performance metric depends on the specific problem, the characteristics of the dataset, and the desired behavior of the model. For example, accuracy is a suitable metric for balanced classification problems, while precision and recall become more relevant when dealing with imbalanced classes. For regression tasks, MSE, RMSE, and MAE are commonly used, with RMSE providing a more interpretable metric.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc46dcfb-adde-4613-b911-650af9fddb23",
   "metadata": {},
   "source": [
    "## 5) What is the curse of dimensionality in KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffb1e6e-4372-42de-95a4-af48927c177f",
   "metadata": {},
   "source": [
    "### => The curse of dimensionality is a phenomenon that occurs in machine learning algorithms, including k-Nearest Neighbors (KNN), when the number of features (dimensions) in the dataset becomes very large. It refers to the adverse effects that arise when working in high-dimensional spaces, where the data points are spread thinly across the feature space.\n",
    "\n",
    "### => In KNN, the algorithm determines the class or value of a data point based on the majority class or average of its k-nearest neighbors in the feature space. The distance between data points becomes a crucial factor in this algorithm, as it measures how similar or dissimilar two data points are. However, as the number of dimensions increases, the volume of the space grows exponentially, and the data points tend to become more sparse.\n",
    "\n",
    "### The curse of dimensionality in KNN leads to the following problems:\n",
    "\n",
    "## 1) Increased computational complexity: \n",
    "### => As the number of dimensions grows, the distance calculations between data points become more computationally expensive. Finding the nearest neighbors in high-dimensional spaces becomes increasingly time-consuming.\n",
    "\n",
    "## 2) Degraded accuracy:\n",
    "### => In high-dimensional spaces, the concept of distance becomes less meaningful. The distances between data points become more uniform, and the concept of \"closeness\" becomes less relevant. Consequently, KNN may lose its ability to distinguish meaningful patterns or relationships in the data.\n",
    " \n",
    "## 3) Increased data requirements:\n",
    "### => To maintain the same level of representation and avoid having insufficient data points in each region of the feature space, you would need an exponentially larger dataset as the number of dimensions increases. In practice, collecting and managing such extensive datasets can be challenging.\n",
    "\n",
    "## 4) Curse of locality:\n",
    "### => In high-dimensional spaces, most data points appear to be equidistant from each other, leading to the \"curse of locality.\" It means that most data points become equally relevant in the k-nearest neighbor calculations, making the results less reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764a3107-d7e0-4bb5-aa3a-1812f912d4cf",
   "metadata": {},
   "source": [
    "## 6) How do you handle missing values in KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930e1b55-2497-4596-8999-2916010b7f4e",
   "metadata": {},
   "source": [
    "## 1) Dropping instances:\n",
    "### => If the dataset has missing values for only a small number of instances, one option is to remove those instances from the dataset. However, this approach might not be ideal if the missing values occur in essential areas of the data, or if removing instances leads to a significant loss of information.\n",
    "\n",
    "## 2) Imputation:\n",
    "### => Imputation involves filling in the missing values with estimated values. One common approach is to replace missing values with the mean, median, or mode of the corresponding feature. This method assumes that the missing values are missing at random and that the overall distribution of the data is preserved.\n",
    "\n",
    "## 3) KNN imputation:\n",
    "### => This technique involves using KNN to predict the missing values based on the values of the k-nearest neighbors. For each missing value, the algorithm finds the k-nearest neighbors of the instance with the missing value and takes the average (for numerical features) or majority vote (for categorical features) of those neighbors' values to fill in the missing value.\n",
    "\n",
    "## 4) Interpolation methods: \n",
    "### => For time-series or sequential data, interpolation methods like linear interpolation or cubic spline interpolation can be used to estimate missing values based on the values of neighboring instances.\n",
    " \n",
    "## 5) Predictive models:\n",
    "### => If the dataset has a large number of missing values and the missingness is not entirely random, you can use predictive models like decision trees or regression models to predict the missing values based on the available data.\n",
    "\n",
    "## 6) Create a missing value indicator: \n",
    "### => Instead of imputing the missing values directly, you can create an additional binary feature indicating whether a particular value is missing or not. This way, the algorithm can learn the importance of the missingness as a separate feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be38f29-dd19-4a14-9bfa-4fdab2bc2ff3",
   "metadata": {},
   "source": [
    "## 7) Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb5f3fa-f47b-43e2-b87e-cd599cf50361",
   "metadata": {},
   "source": [
    "## 1) **KNN Classifier:**\n",
    "\n",
    "##  Task:\n",
    "### => In classification, KNN is used to assign a class label to a data point based on the majority class among its k-nearest neighbors.\n",
    "##  Output:\n",
    "### => The output of KNN classifier is a discrete class label.\n",
    "##  Distance Metric:\n",
    "### => KNN classifiers typically use distance metrics like Euclidean distance, Manhattan distance, or Minkowski distance to measure the similarity between data points.\n",
    "##  Evaluation:\n",
    "### => Common evaluation metrics for classification include accuracy, precision, recall, F1-score, and area under the Receiver Operating Characteristic (ROC) curve.\n",
    "##  Handling outliers:\n",
    "### => KNN classifiers can be sensitive to outliers as they can affect the decision boundaries.\n",
    "## 2) **KNN Regressor:**\n",
    "\n",
    "##  Task:\n",
    "### => In regression, KNN is used to predict a continuous value for a data point based on the average or weighted average of the target values among its k-nearest neighbors.\n",
    "##  Output:\n",
    "### => The output of KNN regressor is a continuous value.\n",
    "##  Distance Metric: \n",
    "### => Similar to the classifier, KNN regressors also use distance metrics to determine the neighbors and calculate the predicted value.\n",
    "##  Evaluation:\n",
    "### => Common evaluation metrics for regression include Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and R-squared (coefficient of determination).\n",
    "##  Handling outliers:\n",
    "### => KNN regressors are less affected by outliers as the final prediction is based on averaging the target values.\n",
    "## **Choosing KNN Classifier or Regressor:**\n",
    "\n",
    "## For Classification Problems:\n",
    "### =>KNN classifier is better suited when the target variable is categorical or discrete and you want to assign class labels to data points based on the majority class of their neighbors. It can be useful for problems like image recognition, text categorization, or sentiment analysis.\n",
    "## For Regression Problems: \n",
    "### =>KNN regressor is better when the target variable is continuous, and you want to predict numeric values based on the average or weighted average of neighboring points. It can be useful in problems like predicting house prices, stock prices, or demand forecasting.\n",
    "## **Considerations:**\n",
    "\n",
    "### => The choice between KNN classifier and regressor also depends on the nature of the data and the number of neighbors (k) chosen. Generally, smaller k values lead to more flexible models that can capture local patterns but might be sensitive to noise. Larger k values result in smoother models but may lose some local details.Preprocessing, feature scaling, and handling missing values are essential for both KNN classifier and regressor to ensure meaningful distance calculations and accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30bb1b0-87df-4e38-ac3c-83fb090fa6ff",
   "metadata": {},
   "source": [
    "## 8) What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad20a3db-18df-49b1-b1b1-f0690d49712b",
   "metadata": {},
   "source": [
    "## **Strengths of KNN Algorithm:**\n",
    "\n",
    "## 1) Simple and Intuitive: \n",
    "### => KNN is straightforward to understand and implement. It doesn't make any assumptions about the underlying data distribution, making it a non-parametric method.\n",
    "\n",
    "## 2) No Training Phase:\n",
    "### => KNN is a lazy learning algorithm, meaning it doesn't have a separate training phase. The model memorizes the entire dataset, making it easy to update the model with new data without retraining.\n",
    "\n",
    "## 3) Flexibility in Distance Metrics:\n",
    "### => KNN allows using various distance metrics to measure similarity between data points, such as Euclidean distance, Manhattan distance, and others. This flexibility makes it adaptable to different types of data and problem domains.\n",
    "\n",
    "## 4) Adapts to Complex Decision Boundaries:\n",
    "### => KNN can capture complex decision boundaries by adjusting the number of neighbors (k) based on the data distribution. It can handle non-linear relationships between features and the target.\n",
    "\n",
    "## **Weaknesses of KNN Algorithm:**\n",
    "\n",
    "## 1) Computational Complexity:\n",
    "### => KNN's main drawback is its high computational cost, especially on large datasets. The algorithm requires calculating distances between the query point and all training points, which can be time-consuming for large feature spaces.\n",
    "\n",
    "## 2) Curse of Dimensionality:\n",
    "### => As the number of dimensions increases, KNN becomes less effective due to the curse of dimensionality. In high-dimensional spaces, the data becomes sparse, and distances between data points lose their meaning.\n",
    "\n",
    "## 3) Sensitive to Noise and Outliers:\n",
    "### => Outliers or noisy data can significantly influence the predictions in KNN, leading to inaccurate results. The algorithm may incorrectly assign class labels or predict target values based on noisy neighbors.\n",
    "\n",
    "## 4) Choosing Optimal k: \n",
    "### => The choice of the number of neighbors (k) is critical in KNN. A small k may result in a noisy model, while a large k can lead to oversmoothed decision boundaries, causing loss of local patterns.\n",
    "\n",
    "## **Addressing KNN's Weaknesses:**\n",
    "\n",
    "## 1) Distance Optimization:\n",
    "### => Approximate nearest neighbor search algorithms, such as KD-trees or Ball trees, can be used to speed up the distance calculations and reduce the computational burden, especially for large datasets.\n",
    "\n",
    "## 2) Feature Selection and Dimensionality Reduction: \n",
    "### => To mitigate the curse of dimensionality, feature selection or dimensionality reduction techniques like Principal Component Analysis (PCA) or t-distributed Stochastic Neighbor Embedding (t-SNE) can be applied to retain the most informative features.\n",
    "\n",
    "## 3) Handling Outliers: \n",
    "### => Outlier detection methods or robust distance metrics (e.g., Mahalanobis distance) can be employed to reduce the impact of outliers.\n",
    "\n",
    "## 4) Optimizing k: \n",
    "### => Cross-validation or grid search techniques can help determine the optimal value of k. Different values of k can be tested, and the one with the best performance can be chosen.\n",
    "\n",
    "## 5) Distance Weights: \n",
    "### => In regression tasks, assigning weights to the neighbors based on their distance can be helpful to give more importance to closer neighbors in the prediction.\n",
    "\n",
    "## 6) Ensemble Methods:\n",
    "### => Combining multiple KNN models or using ensemble techniques like Bagging or Boosting can improve prediction accuracy and reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb6453c-9b56-4b54-8cc3-32847e094a96",
   "metadata": {},
   "source": [
    "## 9) What is the difference between Euclidean distance and Manhattan distance in KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c2ed38-d0f7-4960-be64-fe06b1f78b21",
   "metadata": {},
   "source": [
    "## **Euclidean Distance:**\n",
    "\n",
    "## 1) Formula:\n",
    "### => The Euclidean distance between two points (p1 and p2) in a multi-dimensional space is calculated as the straight-line distance between them, considering the square root of the sum of squared differences of their corresponding coordinates.\n",
    "## 2) Formula (2D space):\n",
    "### => √((x2 - x1)^2 + (y2 - y1)^2)\n",
    "## 3) Formula (n-dimensional space):\n",
    "### => √(Σ(xi - yi)^2) for all dimensions i = 1 to n\n",
    "## 4) Properties: \n",
    "### => Euclidean distance is always non-negative, and it satisfies the triangle inequality, meaning that the direct path between two points is always shorter than any detour.\n",
    "## 5) Interpretation: \n",
    "### => Euclidean distance measures the length of the shortest path between two points, assuming that you can move in any direction and at any angle.\n",
    "## **Manhattan Distance:**\n",
    "\n",
    "## 1) Formula: \n",
    "### => The Manhattan distance (also known as city block distance or L1 norm) between two points (p1 and p2) in a multi-dimensional space is calculated as the sum of the absolute differences of their corresponding coordinates.\n",
    "## 2) Formula (2D space): \n",
    "### => |x2 - x1| + |y2 - y1|\n",
    "## 3) Formula (n-dimensional space):\n",
    "### => Σ|xi - yi| for all dimensions i = 1 to n\n",
    "## 4) Properties:\n",
    "### => Manhattan distance is also always non-negative and satisfies the triangle inequality.\n",
    "## 5) Interpretation:\n",
    "### => Manhattan distance measures the distance between two points by considering only vertical and horizontal movements, as if you were moving through a grid of city blocks in a city.\n",
    "## **Comparison:**\n",
    "\n",
    "\n",
    "### => Euclidean distance tends to give more weight to differences in the larger dimensions, while Manhattan distance weighs all dimensions equally.\n",
    "\n",
    "### => Euclidean distance is useful when you want to measure the straight-line distance between points in continuous spaces.\n",
    "\n",
    "### => Manhattan distance is particularly useful when dealing with data that has a grid-like structure, such as images or text data, and when you want to measure distance in terms of moves along the axes.\n",
    "\n",
    "### => The choice between Euclidean distance and Manhattan distance in KNN depends on the nature of the data and the underlying problem. In some cases, one distance metric may perform better than the other, so it is common to try both and use cross-validation to determine the most suitable metric for a specific task.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc21316d-729b-4f76-addd-45f776c9d62e",
   "metadata": {},
   "source": [
    "## 10) What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e09ee40-f31f-490d-bf93-11817c185970",
   "metadata": {},
   "source": [
    "## 1) Distance Calculation:\n",
    "### => KNN determines the similarity between data points based on the distance metric, typically using Euclidean distance, Manhattan distance, or other distance functions. The distance between two data points is a fundamental measure of their similarity or dissimilarity.\n",
    "\n",
    "## 2) Equal Influence of Features:\n",
    "### => For KNN to work effectively, all features should have equal influence in the distance calculations. If one feature has a much larger scale than others, it will contribute more to the overall distance, making the algorithm heavily biased towards that feature.\n",
    "### \n",
    "### By applying feature scaling, you can address these issues and create a more balanced and meaningful distance calculation process:\n",
    "\n",
    "## 1) Normalization (Min-Max Scaling): \n",
    "### => It scales the feature values to a specific range, often between 0 and 1. This method is particularly useful when the features have different ranges, and you want to ensure they all lie within a common scale.\n",
    "\n",
    "## 2) Standardization (Z-score Scaling):\n",
    "### => It transforms the feature values to have a mean of 0 and a standard deviation of 1. Standardization maintains the relative distances between data points and is robust to outliers.\n",
    " \n",
    "## 3) Other Scaling Methods:\n",
    "### => There are other scaling techniques, such as Robust Scaling, where the median and interquartile range are used to scale the features, which are more robust to outliers.\n",
    "\n",
    "### \n",
    "### Benefits of Feature Scaling in KNN:\n",
    "\n",
    "## 1) Improves Distance Calculation: \n",
    "### => Feature scaling ensures that all features contribute equally to the distance calculations. It prevents dominant features from overshadowing other features in the distance computation.\n",
    "\n",
    "## 2) Faster Convergence:\n",
    "### => By scaling the features, KNN is more likely to converge faster during the optimization process, which can lead to quicker and more efficient predictions.\n",
    "\n",
    "## 3) Better Model Performance:\n",
    "### => Proper feature scaling can lead to improved model performance, as KNN becomes less sensitive to variations in feature scales and can better capture the underlying patterns in the data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fc3093-e595-4ba0-94ea-f9eb1efe5720",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
