{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ea0d020-2279-488d-85e7-aa9b42b55537",
   "metadata": {},
   "source": [
    "# 1] What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1fbb8c-9856-4bf4-a300-6a5ac75a9a88",
   "metadata": {},
   "source": [
    "### => Min-Max scaling, also known as normalization, is a data preprocessing technique used to rescale the features of a dataset to a specific range. The goal of this technique is to transform the values of the features so that they fall within a specific range, usually between 0 and 1\n",
    "### \n",
    "### => X_scaled = (X - X_min) / (X_max - X_min)\n",
    "### \n",
    "### => Area (in square feet): ranging from 500 to 3000\n",
    "### Scaled Area = (Area - 500) / (3000 - 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a08df8-17c2-48f4-8a3b-f3daaada5c9a",
   "metadata": {},
   "source": [
    "# 2] What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3c7f6d-ee1b-4026-b2cb-5ec593909390",
   "metadata": {},
   "source": [
    "### => Unit vector scaling, also known as normalization, is a data preprocessing technique used to rescale the features of a dataset to have unit length. The goal of this technique is to transform the values of the features so that they fall within the range [-1, 1] or [0, 1] and have the same scale.\n",
    "### \n",
    "### => X_scaled = X / ||X||\n",
    "### \n",
    "### => The difference between Unit vector scaling and Min-Max scaling is that the former rescales the feature vectors to have unit length, while the latter rescales the feature values to a specific range between 0 and 1. Unit vector scaling preserves the direction of the original feature vectors, while Min-Max scaling changes their direction in some cases.\n",
    "### \n",
    "### => Area (in square feet): ranging from 500 to 3000\n",
    "### Euclidean norm of the Area vector: sqrt(500^2 + 3000^2) = 3010.99\n",
    "### Scaled Area = Area / 3010.99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8379fb94-c83c-4222-aeea-e692b502d61c",
   "metadata": {},
   "source": [
    "# 3] What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfa7009-34e1-4b11-a177-db65827088a9",
   "metadata": {},
   "source": [
    "### => PCA, or Principal Component Analysis, is a widely used technique for dimensionality reduction. It is a statistical method that transforms high-dimensional data into a lower-dimensional space while retaining as much of the original information as possible.\n",
    "\n",
    "### => The goal of PCA is to find a new set of variables, called principal components, that capture the maximum amount of variation in the data. These principal components are a linear combination of the original variables and are orthogonal to each other. The first principal component accounts for the most variation in the data, followed by the second, and so on.\n",
    "\n",
    "### => PCA is useful in reducing the dimensionality of data because it allows us to discard the least important variables and focus on the most important ones. This can be particularly useful when working with high-dimensional data that can be difficult to visualize or analyze.\n",
    "### \n",
    "### => Suppose we have a dataset containing the following variables: height, weight, age, and income. We want to reduce the dimensionality of this dataset while retaining as much information as possible.\n",
    "\n",
    "### => First, we standardize the data to have a mean of zero and a standard deviation of one. Then, we use PCA to find the principal components of the dataset. Let's say that the first two principal components capture 90% of the variation in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62922820-2d11-48b2-80ad-e0019922819e",
   "metadata": {},
   "source": [
    "# 4] What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77294bf9-3773-4094-9992-7ac0fba23cde",
   "metadata": {},
   "source": [
    "### => Feature Extraction is the process of selecting and transforming a set of input features into a new set of features that better represents the underlying patterns in the data. It is a crucial step in many machine learning and data analysis tasks, such as classification, clustering, and visualization.\n",
    "\n",
    "### => PCA can be used for Feature Extraction by finding the principal components of a dataset and using them as the new set of features. The principal components represent the directions of maximum variance in the data and capture the most important patterns or structures in the data.\n",
    "### \n",
    "### => Suppose we have a dataset containing images of handwritten digits, and each image is represented as a vector of pixel intensities. We want to classify these images into different digits, but the high dimensionality of the data makes it difficult to do so.\n",
    "\n",
    "### => We can use PCA to extract features from the images by finding the principal components of the dataset. Each principal component represents a combination of pixel intensities that captures the most important variations in the images.\n",
    "\n",
    "### => We can then use the first few principal components as the new set of features and train a classifier on this reduced set of features. By using PCA for Feature Extraction, we are able to reduce the dimensionality of the data and capture the most important patterns, which can improve the accuracy and efficiency of the classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3632d8-386d-4014-bd69-ad62ac38a6ce",
   "metadata": {},
   "source": [
    "# 5] You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a647ff86-a54e-41c7-a055-dd7f820051fc",
   "metadata": {},
   "source": [
    "### => Min-Max scaling is a data preprocessing technique that rescales the values of a feature to a range between 0 and 1. It is a common normalization technique that is useful when the features have different scales and ranges.\n",
    "### \n",
    "### => To use Min-Max scaling to preprocess the data, we would first compute the minimum and maximum values of each feature in the dataset. Then, for each value in the feature, we would subtract the minimum value and divide by the range (i.e., the difference between the maximum and minimum values)\n",
    "### \n",
    "### => For example, suppose the price feature has a minimum value of 5 dollars and a maximum value of 50 dollars, and we want to scale the value of 20 dollars. We would first subtract the minimum value (i.e., 5 dollars) to get 15 dollars, then divide by the range (i.e., 45 dollars) to get 0.33. Therefore, the scaled value of 20 dollars would be 0.33.\n",
    "\n",
    "### => We would repeat this process for each value in each feature in the dataset, rescaling the values to a range between 0 and 1. This ensures that each feature has the same scale and range, which can be useful for some machine learning algorithms that rely on distance metrics or numerical optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87a3834-f8de-4ffd-9dd1-825d49de02c6",
   "metadata": {},
   "source": [
    "# 6] You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eecd93d-a9ae-4595-8c6f-5f03c8cb90e3",
   "metadata": {},
   "source": [
    "## 1) Standardize the data:\n",
    "### => We would first standardize the data by subtracting the mean and dividing by the standard deviation of each feature. This ensures that each feature has a mean of zero and a standard deviation of one, which can be useful for some machine learning algorithms.\n",
    "\n",
    "## 2) Compute the principal components:\n",
    "### => We would then compute the principal components of the standardized data using PCA. The principal components are orthogonal directions in the high-dimensional space of the data that capture the most important patterns or structures. The first principal component explains the most variance in the data, followed by the second principal component, and so on.\n",
    "\n",
    "## 3) Select the number of principal components: \n",
    "### => We would then select the number of principal components to keep based on the amount of variance explained by each component. We can plot the cumulative variance explained by the principal components and choose the number of components that explain a significant portion of the variance.\n",
    "\n",
    "## 4) Transform the data:\n",
    "### => Finally, we would transform the original features into the new set of principal components using the selected number of components. This reduces the dimensionality of the dataset, while retaining the most important patterns or structures in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928b57be-d867-4c23-aea8-96b0ddc1e0de",
   "metadata": {},
   "source": [
    "# 7] For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f26137b-f352-43f1-ab2a-81901f4197b7",
   "metadata": {},
   "source": [
    "### => [0, 0.2105, 0.4736, 0.7894, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c7c65b-2300-4838-953b-b3f59fbae84d",
   "metadata": {},
   "source": [
    "# 8] For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6068ab8-922f-4fc1-92bc-d5ce5c048519",
   "metadata": {},
   "source": [
    "### => height and weight are likely to be strongly correlated, as are age and blood pressure. Gender is a categorical variable that may not contribute much to the variance in the data. Therefore, we may expect that the first principal component would capture the variance associated with height and weight, while the second principal component would capture the variance associated with age and blood pressure.\n",
    "\n",
    "### => Assuming that the first two principal components explain a significant portion of the variance in the data, we would choose to retain two principal components for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ca93ec-66d7-415e-879f-6a4a22a950f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
