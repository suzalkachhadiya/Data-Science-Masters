{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3731cca0-7580-4725-885a-93e2e3acdccd",
   "metadata": {},
   "source": [
    "# 1] What is anomaly detection and what is its purpose?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2a8f6d-3365-43d8-87b2-758abf51c690",
   "metadata": {},
   "source": [
    "### => Anomaly detection is the process of identifying data points, events or observations that deviate significantly from the norm. The main purposes of anomaly detection are:\n",
    "\n",
    "## 1) Identifying outliers: \n",
    "### => Anomaly detection can help identify outliers that are due to errors, faults or abnormalities in the data. This allows for correcting or removing the outliers before they negatively impact model training or predictions.\n",
    "## 2) Detecting novelties: \n",
    "### => It can allow the discovery of unknown phenomena and new patterns in data that do not conform to expected behavior. This includes detecting fraud, network intrusions, breakdowns, etc.\n",
    "## 3) Enhancing robustness: \n",
    "### => Anomaly detection improves the robustness of models by detecting examples that differ from the majority. Models trained on clean, regular data are less likely to fail on new data.\n",
    "## 4) Identifying change: \n",
    "### => It enables the monitoring of changes in behavior over time to detect if a process or system has deviated from its baseline. This allows identifying if something fundamental has changed.\n",
    "## 5) Providing insights: \n",
    "### => Understanding anomalies and their underlying causes can provide useful business insights and opportunities for improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91a58e0-a4f2-4588-95b0-d48ba1030ce1",
   "metadata": {},
   "source": [
    "# 2] What are the key challenges in anomaly detection?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72325348-7b16-40fd-ae26-d3034df88945",
   "metadata": {},
   "source": [
    "## 1) Defining normal: \n",
    "### => It can be difficult to precisely define what constitutes \"normal\" behavior especially in complex systems. Without a good sense of normal, it's hard to identify anomalies.\n",
    "## 2) Imbalanced data:\n",
    "### => Anomalies by definition are rare events compared to normal data. The extreme imbalance between anomalies and normal data makes modeling challenging.\n",
    "## 3) Masking:\n",
    "### => Anomalies may mask or absorb into large clusters of normal data points. This makes them difficult to discern from surrounding data.\n",
    "## 4) Data quality:\n",
    "### => Poor data quality such as missing values, noise, outliers etc. can substantially increase false positives and negatives. Data preprocessing is critical.\n",
    "## 5) Context dependence:\n",
    "### => Anomalies are often context specific and what is anomalous in one scenario may be perfectly normal in another. Accounting for context is challenging.\n",
    "## 6) Concept drift: \n",
    "### => In dynamic systems, the concept of normal behavior can itself drift over time requiring the models to be updated.\n",
    "## 7) Interpretability: \n",
    "### => It's often not enough just to detect anomalies, but also critical to understand why a data point is anomalous. Lack of interpretability makes debugging and root cause analysis tough.\n",
    "## 8) Evaluation:\n",
    "### => Defining good evaluation metrics and getting annotated anomaly data for validation is difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61602ab5-6a4f-400c-8566-0a77f0b0719e",
   "metadata": {},
   "source": [
    "# 3] How does unsupervised anomaly detection differ from supervised anomaly detection?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8e28f7-20bc-46d2-8ba2-089344dd4ea1",
   "metadata": {},
   "source": [
    "## 1) Training data:\n",
    "### => Unsupervised anomaly detection does not require labeled or annotated data. It relies only on unlabeled normal data.\n",
    "### => Supervised requires data labeled as normal and anomalous to train models.\n",
    "## 2) Assumptions:\n",
    "### => Unsupervised methods assume anomalies are rare and different from normal instances.\n",
    "### => Supervised methods make no assumptions on data distribution.\n",
    "## 3) Models:\n",
    "### => Unsupervised methods typically rely on statistical models or unsupervised ML models like clustering.\n",
    "### => Supervised methods leverage classification algorithms, neural networks etc.\n",
    "## 4) Detection:\n",
    "### => Unsupervised methods detect anomalies by identifying points deviating from clusters or density estimates of normal data.\n",
    "### => Supervised models detect anomalies based on learned decision boundaries between normal and anomalous classes.\n",
    "## 5) Performance:\n",
    "### => Unsupervised methods generally have higher false positives due to reliance on assumptions.\n",
    "### => Supervised models can leverage labeled data to improve detection accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37edf84a-3fd6-4597-9b0f-5fcbbfe739b1",
   "metadata": {},
   "source": [
    "# 4] What are the main categories of anomaly detection algorithms?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123a9fb6-c25d-4809-bc24-4620d02e723b",
   "metadata": {},
   "source": [
    "## 1) Statistical models: \n",
    "### => These assume data is generated from a statistical distribution and identify anomalies as deviations from that distribution. Examples include Gaussian mixture models and multivariate Gaussians.\n",
    "## 2) Proximity-based models:\n",
    "### => These use distance or density measures to identify anomalies that are far from their neighbors. Examples include k-nearest neighbors and local outlier factor algorithms.\n",
    "## 3) Clustering models:\n",
    "### => These rely on clustering algorithms like k-means to group similar data points. Points not belonging to clusters are flagged as anomalies.\n",
    "## 4) Classification models:\n",
    "### => These leverage classification algorithms like SVM and random forest trained on labeled data to classify new points as anomalous or normal.\n",
    "## 5) Neural networks:\n",
    "### => Models like autoencoders and deep belief networks are used to learn representations of normal data. Reconstruction errors are used to detect anomalies.\n",
    "## 6) Information theoretic models:\n",
    "### => These detect anomalies based on metrics like entropy and Kolmogorov complexity which quantify information content of data points.\n",
    "## 7) Ensemble models: \n",
    "### => Combinations of multiple models can be used together to improve anomaly detection through techniques like stacking and voting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f659599d-8d82-4a6c-a515-93fcfb180631",
   "metadata": {},
   "source": [
    "# 5] What are the main assumptions made by distance-based anomaly detection methods?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a511d0f2-f33c-4084-88a5-36a49b421dea",
   "metadata": {},
   "source": [
    "### => Anomalies are rare occurrences within the data. Typically it is assumed anomalies make up a very small percentage (1-5%) of the overall data.\n",
    "### => Normal instances lie close to their closest neighbors while anomalies are far away. The distance between a normal data point and its k-nearest neighbors will be small compared to the distances for anomalous points.\n",
    "### => Normal data points belong to dense neighborhoods and clusters. The local density around most normal points is assumed to be higher than that around anomalies.\n",
    "### => Similar instances have small pairwise distances while anomalies are dissimilar to normal points. The distance metrics used accurately capture the notion of similarity for the data.\n",
    "### => The boundaries between normal and anomalous regions are defined by the distances of points from their neighbors. Thresholds based on these distances can segregate anomalies from normal instances.\n",
    "### => The distance metrics generalize to unseen data. Distance measures that work well on training data will also be meaningful for new test data.\n",
    "### => There are no camouflage anomalies. Anomalies do not take on values that mask their presence within clusters of normal data points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50037d13-d4b5-4962-a9e4-bd35d0eabb9f",
   "metadata": {},
   "source": [
    "# 6] How does the LOF algorithm compute anomaly scores?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969a7cc5-b99f-4f1d-bf27-f207ef3a5d43",
   "metadata": {},
   "source": [
    "### => For each point, identify its k-nearest neighbors (the local neighborhood).\n",
    "### => Calculate the reachability distance of each point to its neighbors. This is the maximum of the k-distance (distance to kth neighbor) of the two points.\n",
    "### => Compute local reachability density (LRD) which is inverse of average reachability distance of a point to its neighbors.\n",
    "### => Calculate local outlier factor (LOF) for each point which is ratio of average LRD of its neighbors and its own LRD.\n",
    "### => Points with higher LOF scores have lower density than their neighbors and are considered anomalies.\n",
    "### \n",
    "### Intuitively, anomalies have lower local density and are more isolated. LOF captures this by comparing the relative densities. It doesn't require knowing the global data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c07f4e9-8b50-4944-8c7b-3de61c2ea377",
   "metadata": {},
   "source": [
    "# 7] What are the key parameters of the Isolation Forest algorithm?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a049dcd4-b498-4d86-ad45-dd17aa4450d1",
   "metadata": {},
   "source": [
    "## 1) n_estimators \n",
    "### => The number of isolation trees to construct in the ensemble. More trees leads to better accuracy but higher computing cost. Typical values range from 10 to 100.\n",
    "## 2) max_samples\n",
    "### => The number of samples to draw from the dataset when constructing each isolation tree. Smaller values result in more random trees, reducing variance but increasing bias. Typical range is between 50 to 300.\n",
    "## 3) max_features \n",
    "### => The number of features to consider when determining splits on isolation tree nodes. Can be an integer or float in [0,1]. Lower values make the model more random but may improve anomaly detection.\n",
    "## 4) random_state \n",
    "### => The seed used to initialize the random number generator for reproducibility.\n",
    "## 5) n_jobs \n",
    "### => The number of CPU cores to use for parallel processing. -1 uses all available cores.\n",
    "## 6) contamination \n",
    "### => The expected proportion of anomalies in the data, used to tune threshold on the anomaly scores.\n",
    "## 7) max_depth \n",
    "### => Maximum depth of each isolation tree. Does not normally need tuning from the default of log2(n_samples)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ab195a-691b-47f6-a01c-86aab6661fbd",
   "metadata": {},
   "source": [
    "# 8] If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score using KNN with K=10?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe1f6f7-951a-442e-9e3c-9929421b27e6",
   "metadata": {},
   "source": [
    "\n",
    "### =>In KNN anomaly detection, the anomaly score for a data point is typically calculated based on the distance to its Kth nearest neighbor.\n",
    "\n",
    "### =>Since K=10 in this case, we are interested in the distance to the 10th nearest neighbor.\n",
    "\n",
    "### =>The question states that within a radius of 0.5, there are only 2 neighbors of the same class.\n",
    "\n",
    "### =>This means the distance to the 10th nearest neighbor must be greater than 0.5.\n",
    "\n",
    "### =>As the anomaly score is based on this distance, with a higher distance implying a higher chance of being an anomaly, the anomaly score for this point will be relatively high.\n",
    "\n",
    "### =>Without knowing the exact distance to the 10th neighbor, it is not possible to calculate the precise anomaly score.\n",
    "\n",
    "### =>But we can conclude that having only 2 neighbors within a radius of 0.5 when K=10, results in this data point likely having a high anomaly score and low local density compared to its neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54fb86e-a161-4acb-9430-2d9ee75e6680",
   "metadata": {},
   "source": [
    "# 9] Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the anomaly score for a data point that has an average path length of 5.0 compared to the average path length of the trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "182e70cb-aa98-42af-a46a-baf8e1ddbce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fb7b3f7-f8b1-40ed-94ad-6c20f911fec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomaly_score(n,average_path_length):\n",
    "    H=2*(math.log(n-1)) + 0.5772156649\n",
    "    c= 2*H - ((2*(n-1))/n)\n",
    "    score= 1/(2**(average_path_length / c))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32fc614c-bc2c-46dd-b16f-0ae9f828562b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8947998126198259"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score=anomaly_score(3000,5.0)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30099918-5de0-4c68-8aa9-9e547a00651b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
