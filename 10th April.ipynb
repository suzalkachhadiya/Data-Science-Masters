{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74a46c23",
   "metadata": {},
   "source": [
    "# 1] A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef393cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6086956521739131"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.4 * 0.7) / ((0.6 * 0.3) + (0.4 * 0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c26f05a",
   "metadata": {},
   "source": [
    "# 2] What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096c12c1",
   "metadata": {},
   "source": [
    "## 1) Feature Type:\n",
    "\n",
    "### => Bernoulli Naive Bayes: This variant assumes that features are binary or follow a Bernoulli distribution. Each feature is treated as a binary indicator of presence or absence of a particular attribute.\n",
    "### => Multinomial Naive Bayes: It is designed for discrete features that represent counts or frequencies, such as word frequencies in text classification or occurrence counts of events in a fixed-size set.\n",
    "## 2) Feature Representation:\n",
    "\n",
    "### => Bernoulli Naive Bayes: Each feature is represented as a binary value (0 or 1), indicating the absence or presence of the feature.\n",
    "### => Multinomial Naive Bayes: Features are represented as discrete counts or frequencies. For example, in text classification, each feature may represent the number of times a word appears in a document or the frequency of a word in a corpus.\n",
    "## 3) Probability Estimation:\n",
    "\n",
    "### => Bernoulli Naive Bayes: The conditional probabilities are estimated using the frequency of feature occurrence within each class. The probability of a feature being present (1) or absent (0) is calculated.\n",
    "### => Multinomial Naive Bayes: The conditional probabilities are estimated using the frequency or counts of each feature within each class. The probability of a feature taking a specific value (count) is computed.\n",
    "## 4) Handling Missing Features:\n",
    "\n",
    "### => Bernoulli Naive Bayes: It can handle missing features by treating them as absent (0).\n",
    "### => Multinomial Naive Bayes: Missing features are typically ignored in the computation of conditional probabilities, assuming they have zero counts.\n",
    "## 5) Application Domains:\n",
    "\n",
    "### => Bernoulli Naive Bayes: It is commonly used in text classification tasks where features represent the presence or absence of specific words or features in documents.\n",
    "### => Multinomial Naive Bayes: It is widely used in text classification, sentiment analysis, and other applications involving discrete feature counts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4d9da7",
   "metadata": {},
   "source": [
    "# 3] How does Bernoulli Naive Bayes handle missing values?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a7f0db",
   "metadata": {},
   "source": [
    "### => Bernoulli Naive Bayes handles missing values by treating them as absent (0). In this variant of Naive Bayes, each feature is represented as a binary value, indicating the presence or absence of a particular attribute. When a feature value is missing, it is considered as if the feature is absent (0).\n",
    "\n",
    "### => By treating missing values as 0, Bernoulli Naive Bayes assumes that the missing values do not contribute to the presence of the feature. This assumption aligns with the Bernoulli distribution, where the feature is represented as a binary indicator.\n",
    "\n",
    "### => When calculating the conditional probabilities in Bernoulli Naive Bayes, the absence of a feature (0) is considered along with the presence (1) within each class. The frequency of feature occurrence is estimated, taking into account the presence or absence of the feature for each class.\n",
    "\n",
    "### => It's important to note that the treatment of missing values as absent (0) may introduce a bias if the missingness is not random or if the missing values carry some specific meaning. In such cases, alternative approaches, such as imputation techniques, may be considered to handle missing values appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12894be7",
   "metadata": {},
   "source": [
    "# 4] Can Gaussian Naive Bayes be used for multi-class classification?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91ba793",
   "metadata": {},
   "source": [
    "### => Yes, Gaussian Naive Bayes can be used for multi-class classification. Gaussian Naive Bayes is a variant of Naive Bayes that assumes a Gaussian (normal) distribution for continuous or real-valued features. It is commonly used when the features are continuous and can be modeled by a Gaussian distribution.\n",
    "\n",
    "### => In the case of multi-class classification, where there are more than two classes, Gaussian Naive Bayes can still be applied. The classifier builds separate Gaussian distribution models for each class based on the training data, estimating the mean and variance for each feature in each class. During prediction, it calculates the probability of the instance belonging to each class using the Gaussian distribution parameters and applies the Bayes' theorem to determine the most probable class.\n",
    "\n",
    "### => The decision rule in Gaussian Naive Bayes assigns the class label with the highest posterior probability given the observed feature values. This process can be extended to multiple classes by comparing the posterior probabilities of each class and selecting the one with the highest probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9dae6e",
   "metadata": {},
   "source": [
    "# 5] Assignment:\n",
    "## Data preparation:\n",
    "### Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message is spam or not based on several input features.\n",
    "## Implementation:\n",
    "### Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the dataset. You should use the default hyperparameters for each classifier.\n",
    "## Results:\n",
    "### Report the following performance metrics for each classifier:\n",
    "### Accuracy\n",
    "### Precision\n",
    "### Recall\n",
    "### F1 score\n",
    "## Discussion:\n",
    "### Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is the case? Are there any limitations of Naive Bayes that you observed?\n",
    "## Conclusion: \n",
    "### Summarise your findings and provide some suggestions for future work.\n",
    "### \n",
    "## Note:\n",
    "### This dataset contains a binary classification problem with multiple features. The dataset is relatively small, but it can be used to demonstrate the performance of the different variants of Naive Bayes on a real-world problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b016362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0d773b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_%3B</th>\n",
       "      <th>char_freq_%28</th>\n",
       "      <th>char_freq_%5B</th>\n",
       "      <th>char_freq_%21</th>\n",
       "      <th>char_freq_%24</th>\n",
       "      <th>char_freq_%23</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.00               0.64           0.64           0.0   \n",
       "1            0.21               0.28           0.50           0.0   \n",
       "2            0.06               0.00           0.71           0.0   \n",
       "3            0.00               0.00           0.00           0.0   \n",
       "4            0.00               0.00           0.00           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.32            0.00              0.00                0.00   \n",
       "1           0.14            0.28              0.21                0.07   \n",
       "2           1.23            0.19              0.19                0.12   \n",
       "3           0.63            0.00              0.31                0.63   \n",
       "4           0.63            0.00              0.31                0.63   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...  char_freq_%3B  char_freq_%28  \\\n",
       "0             0.00            0.00  ...           0.00          0.000   \n",
       "1             0.00            0.94  ...           0.00          0.132   \n",
       "2             0.64            0.25  ...           0.01          0.143   \n",
       "3             0.31            0.63  ...           0.00          0.137   \n",
       "4             0.31            0.63  ...           0.00          0.135   \n",
       "\n",
       "   char_freq_%5B  char_freq_%21  char_freq_%24  char_freq_%23  \\\n",
       "0            0.0          0.778          0.000          0.000   \n",
       "1            0.0          0.372          0.180          0.048   \n",
       "2            0.0          0.276          0.184          0.010   \n",
       "3            0.0          0.137          0.000          0.000   \n",
       "4            0.0          0.135          0.000          0.000   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       3.756                          61   \n",
       "1                       5.114                         101   \n",
       "2                       9.821                         485   \n",
       "3                       3.537                          40   \n",
       "4                       3.537                          40   \n",
       "\n",
       "   capital_run_length_total  class  \n",
       "0                       278      1  \n",
       "1                      1028      1  \n",
       "2                      2259      1  \n",
       "3                       191      1  \n",
       "4                       191      1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"spambase_csv (1).csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b77a77a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop([\"class\"],axis=1)\n",
    "y=df[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02352ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB,BernoulliNB,MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8329caee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50212389",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a181a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_g=GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "496d24ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_b=BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d5dfdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_m=MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19a0efff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df223bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "params={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fecdd197",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gcv=GridSearchCV(estimator=clf_g,param_grid=params,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "631b8e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_bcv=GridSearchCV(estimator=clf_b,param_grid=params,cv=10)\n",
    "clf_mcv=GridSearchCV(estimator=clf_m,param_grid=params,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f6e8cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=GaussianNB(), param_grid={})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_gcv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96627549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=BernoulliNB(), param_grid={})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_bcv.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "857a7430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=MultinomialNB(), param_grid={})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_mcv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e19309c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predg=clf_gcv.predict(X_test)\n",
    "y_predb=clf_bcv.predict(X_test)\n",
    "y_predm=clf_mcv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2e07c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst=[y_predb,y_predg,y_predm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90f01808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96c0d268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.93      0.90       543\n",
      "           1       0.89      0.80      0.85       378\n",
      "\n",
      "    accuracy                           0.88       921\n",
      "   macro avg       0.88      0.87      0.87       921\n",
      "weighted avg       0.88      0.88      0.88       921\n",
      "\n",
      "0.8794788273615635\n",
      "[[506  37]\n",
      " [ 74 304]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.73      0.83       543\n",
      "           1       0.71      0.96      0.82       378\n",
      "\n",
      "    accuracy                           0.82       921\n",
      "   macro avg       0.84      0.84      0.82       921\n",
      "weighted avg       0.86      0.82      0.83       921\n",
      "\n",
      "0.8241042345276873\n",
      "[[398 145]\n",
      " [ 17 361]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       543\n",
      "           1       0.75      0.72      0.73       378\n",
      "\n",
      "    accuracy                           0.78       921\n",
      "   macro avg       0.78      0.77      0.78       921\n",
      "weighted avg       0.78      0.78      0.78       921\n",
      "\n",
      "0.7839305103148752\n",
      "[[451  92]\n",
      " [107 271]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in lst:\n",
    "    print(classification_report(y_test,i))\n",
    "    print(accuracy_score(y_test,i))\n",
    "    print(confusion_matrix(y_test,i))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d91557",
   "metadata": {},
   "source": [
    "### => bernouli naive bayes performed the best, because it specially used for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be8f7d2",
   "metadata": {},
   "source": [
    "## limitations:\n",
    "## 1) Gaussian Naive Bayes: \n",
    "### => This variant assumes that continuous features follow a Gaussian (normal) distribution. The limitations associated with Gaussian Naive Bayes include:\n",
    "\n",
    "### => It assumes that the feature distributions are unimodal and have equal variance. If the actual distributions deviate significantly from these assumptions, the performance may be negatively affected.\n",
    "### => It may struggle with data that has outliers or features that do not follow a Gaussian distribution.\n",
    "## 2) Multinomial Naive Bayes: \n",
    "### => This variant is commonly used for text classification tasks, where features represent word frequencies or occurrences. Some limitations of Multinomial Naive Bayes include:\n",
    "\n",
    "### => It assumes that features are independent and have multinomial distribution. If the features are dependent or have a different distribution, the classifier may not perform optimally.\n",
    "### => It may not handle rare or unseen feature occurrences well since it relies on counting frequencies in the training data.\n",
    "## 3) Bernoulli Naive Bayes: \n",
    "### => This variant is similar to Multinomial Naive Bayes but is specifically designed for binary feature data. Some limitations of Bernoulli Naive Bayes include:\n",
    "\n",
    "### => It assumes that features are independent and have a Bernoulli distribution. If the independence assumption is violated or the features have a different distribution, it may lead to suboptimal results.\n",
    "### => It may struggle with imbalanced datasets or when the occurrence of certain features is rare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cf3d41",
   "metadata": {},
   "source": [
    "## Findings:\n",
    "\n",
    "### => Naive Bayes relies on the strong assumption of feature independence, which may not hold true in real-world scenarios.\n",
    "### => It fails to capture feature interactions, which can be important for accurate classification.\n",
    "### => The algorithm assumes specific feature distributions, and deviations from these assumptions can impact performance.\n",
    "### => The zero probability problem occurs when a feature value is absent in the training data, leading to incorrect classifications.\n",
    "### => Naive Bayes has limited expressive power compared to more complex classifiers.\n",
    "### => Insufficient training data or data sparsity can affect the accuracy of Naive Bayes.\n",
    "### \n",
    "## Suggestions for future work:\n",
    "\n",
    "### => Relaxing the independence assumption: Explore methods to relax the strict assumption of feature independence, such as using feature selection techniques or incorporating feature dependencies into the model.\n",
    "### => Handling feature interactions: Investigate techniques to capture feature interactions, such as using higher-order Naive Bayes models or integrating other machine learning algorithms that can capture complex relationships.\n",
    "### => Robustness to feature distributions: Develop approaches to handle deviations from assumed feature distributions, such as using non-parametric methods or adapting Naive Bayes to handle different types of feature distributions.\n",
    "### => Zero probability problem mitigation: Investigate strategies to address the zero probability problem, such as smoothing techniques like Laplace smoothing or employing more sophisticated probability estimation methods.\n",
    "### => Enhancing model complexity: Explore ensemble methods or hybrid models that combine Naive Bayes with other classifiers to improve its expressive power and capture more complex patterns in the data.\n",
    "### => Handling data scarcity: Investigate techniques to handle data sparsity, such as incorporating prior knowledge or leveraging techniques like data augmentation or transfer learning to improve classification performance with limited training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36585977",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
